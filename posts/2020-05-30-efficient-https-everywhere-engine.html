<!DOCTYPE html><html lang="en" dir="ltr"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>A more efficient matching engine for HTTPS Everywhere RuleSets</title><meta name="description" content="Simplex Sigillum Veri"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="canonical" href="https://remusao.github.io/posts/efficient-https-everywhere-engine.html"><link rel="icon" type="image/x-icon" href="/images/favicon.ico"><style type="text/css">body{color:#2f2f2f;font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,Oxygen-Sans,Ubuntu,Cantarell,'Helvetica Neue',sans-serif;line-height:1.6em;margin:20px auto;max-width:40rem;padding:0 1.5em;text-rendering:optimizeLegibility}h1{line-height:1.1em}h2{border-bottom:1px solid #eaecef;padding-bottom:.5em}h1,h2,h3{padding-top:.7em}.header{color:#555;margin-top:-.8em}.main a{color:#3465a4}.main a:visited{color:#75507b}.main figcaption{color:#696f72;font-style:italic;line-height:2em;text-align:center}.main img{display:block;height:auto;margin:auto;padding-bottom:1em;padding-top:1em;width:90%}.main blockquote{border-left:thin solid #d3d7cf;color:#555;margin:10px;padding:0 0 0 1em}code{background-color:#f6f8fa;font-family:SFMono-Regular,Consolas,Liberation Mono,Menlo,monospace;font-size:14px;line-height:20px;padding:.2em .4em}pre code{display:block;overflow-x:auto}.leave-comment-btn,summary{font-size:16px;font-weight:600}.leave-comment-btn{background-color:#28a745;background-image:linear-gradient(-180deg,#34d058,#28a745 90%);border:1px solid rgba(27,31,35,.2);border-radius:.25em;color:#fff;display:inline-block;margin-top:1.5em;padding:3px 10px;text-decoration:none}.leave-comment-btn:hover{box-shadow:0 1px 3px rgba(0,0,0,.24)}summary{color:#516ae8;cursor:pointer;line-height:1.2rem;margin-top:2rem}.comments-list{list-style:none;padding:0}.comment{background-color:#fff;border:1px solid #d1d5da;border-radius:3px;color:#24292e;font-size:16px;margin:1em 0}.meta{background-color:#f6f8fa;border-bottom:1px solid #d1d5da;color:#586069}.author,.date{text-decoration:none}.author{color:#24292e!important;font-weight:700}.date{color:#586069}.date:hover{color:#0366d6}.content{padding-left:20px}.avatar,.emoji{vertical-align:middle}.avatar{display:inline-block;margin-right:5px}.emoji{height:1.1em;width:1.1em}table{border-collapse:collapse;margin:auto}td,th{border:1px solid #ddd;padding:10px}tr:hover{background-color:#f5f5f5}footer{border-top:thin solid #d3d7cf;margin:1.5em 0}.share ul{text-align:right}.share li{display:inline;padding:0 5px}.share img{width:32px}.main a,header a{text-decoration:none}.hljs{background:#f8f8f8;color:#333;display:block;overflow-x:auto;padding:.5em}.hljs-comment,.hljs-quote{color:#6a737d;font-style:italic}.hljs-keyword,.hljs-selector-tag,.hljs-subst{color:#d73a49}.hljs-literal,.hljs-number,.hljs-tag .hljs-attr,.hljs-template-variable,.hljs-variable{color:#005cc5}.hljs-doctag,.hljs-string{color:#032f62}.hljs-section,.hljs-selector-id,.hljs-title{color:#900}.hljs-subst{font-weight:400}.hljs-class .hljs-title,.hljs-type{color:#6f42c1}.hljs-attribute,.hljs-name,.hljs-tag{color:navy;font-weight:400}.hljs-link,.hljs-regexp{color:#009926}.hljs-bullet,.hljs-symbol{color:#990073}.hljs-built_in,.hljs-builtin-name{color:#0086b3}.hljs-meta{color:#999;font-weight:700}.hljs-deletion{background:#fdd}.hljs-addition{background:#dfd}.hljs-emphasis{font-style:italic}.hljs-strong{font-weight:700}</style></head><body><main><header><h1><a href="../"><span>../</span></a></h1></header><h1>A more efficient matching engine for HTTPS Everywhere RuleSets</h1><section class="header"><div>└─ 2020-05-30 • <em>Reading time: ~27 minutes</em></div></section><article><div class="main"><p><strong>TL;DR:</strong> <em>In this post I describe the results of an experiment showing how matching of HTTPS Everywhere rules can be made <strong>between 4x and 10x more memory-efficient</strong>, initialization of the matching engine reduced to less than <strong>25 milliseconds</strong>, and HTTPS upgrades performed in <strong>0.0029</strong> to <strong>0.0073 milliseconds</strong>, using a different design inspired by modern adblockers, without relying on the Rust/WebAssembly combo (i.e. pure JavaScript).</em></p><p><strong>Disclaimer:</strong> <em>This work was <em>not</em> conducted as part of the HTTPS Everywhere project. My intent when experimenting with rulesets matching was to explore new ways to implement an efficient engine and document my findings. I would of course love it if some of these ideas are used upstream.</em></p><p>Over the last few years, the adoption of HTTPS has continuously increased, reaching 50% of the Web traffic for the <a href="https://letsencrypt.org/stats/#percent-pageloads" target="_blank" rel="noopener noreferrer">first time in 2017</a> and up to <a href="https://almanac.httparchive.org/en/2019/security#transport-layer-security" target="_blank" rel="noopener noreferrer">80% in 2019</a>. Yet, <a href="https://www.eff.org/https-everywhere" target="_blank" rel="noopener noreferrer">according to the EFF</a>: <em>“Many sites on the web [still] offer some limited support for encryption over HTTPS, but make it difficult to use. For instance, they may default to unencrypted HTTP, or fill encrypted pages with links that go back to the unencrypted site.”</em></p><p>For this reason, the EFF started the HTTPS Everywhere project in 2014, providing users with a browser extension able to automatically upgrade connections to HTTPS whenever possible.</p><p>To decide when an upgrade is feasible, the extension relies on a database of <em>rulesets</em> allowing it to know for a given URL if HTTPS is supported. These rules are <a href="https://github.com/EFForg/https-everywhere/commits/master/src/chrome/content/rules" target="_blank" rel="noopener noreferrer">continuously updated</a> to limit breakage and maximize coverage.</p><p>Having spent a fair amount of my time working on <a href="https://github.com/cliqz-oss/adblocker" target="_blank" rel="noopener noreferrer">content blockers</a> in the last few years—especially on the <a href="https://whotracks.me/blog/adblockers_performance_study.html" target="_blank" rel="noopener noreferrer">performance aspect</a>—, I have always been curious about how the rule-matching logic was implemented in HTTPS Everywhere, since the task shares many similarities with adblocking. More recently, I stumbled upon two tickets mentioning <a href="https://github.com/EFForg/https-everywhere/issues/12232" target="_blank" rel="noopener noreferrer">high memory usage</a> and <a href="https://trac.torproject.org/projects/tor/ticket/23719" target="_blank" rel="noopener noreferrer">slow initialization of the extension</a> and decided to have a closer look.</p><p>High memory and CPU usage are problematic for multiple reasons. As HTTPS Everywhere is running in various environments—including potentially low-end mobile phones—it has to perform decently even under limited IO performance, slow CPUs and low amount of memory. Moreover, HTTPS is included in Tor (on both desktop and mobile) where the JIT can be disabled when security settings are maxed out, further degrading performance. Also consider that while loading a page, many components of the browser are competing for resources: parsing HTML, evaluating JavaScript, rendering the page, but also privacy protections such as adblockers, and of course, HTTPS Everywhere. Last but not the least, energy consumption (especially on mobile devices), is not to be ignored: A higher CPU usage means reduced battery life.</p><p>While experimenting, I was wondering if some of the optimizations implemented as part of modern content blockers would make sense in HTTPS Everywhere and if they would improve the overall efficiency. This blog post presents some of the results of this investigation. The following contributions and improvements are presented:</p><ul><li>A new design, inspired by some of the same optimizations implemented in the fastest content blockers, leading to an increased efficiency:<ul><li><strong>~4.7MB</strong> of memory usage (<strong>4x less than the current HTTPS Everywhere</strong> Rust/WebAssembly implementation), further reduced to <strong>~2.1MB</strong> when using an experimental statistical data structure (with ideas to reduce it even more).</li><li>Decision time between <strong>0.0029</strong> and <strong>0.0073 milliseconds</strong> when querying the rulesets with a URL to be upgraded to HTTPS.</li><li>Serialization to and deserialization from a compact binary representation in under <strong>25 milliseconds</strong> and no memory copy.</li></ul></li><li>Design and implementation of a compact index data structure allowing to efficiently retrieve a small subset of rules likely to apply to a given input URL.</li><li>A built-in small string compression implementation inspired by SMAZ which allows to reduce memory usage by up to 60%.</li><li>An experimental statistical data structure allowing an even lower memory usage, at the risk of unlikely collisions.</li><li>An experimentation with a compact trie data structure to attempt reducing memory usage further.</li></ul><h2>The Rules</h2><p>Before digging deeper into the design of the matching engine, let’s briefly describe <a href="https://www.eff.org/https-everywhere/rulesets" target="_blank" rel="noopener noreferrer">HTTPS Everwhere rulesets</a>.</p><p>The database of rules is made of <em>thousands of rulesets</em> (currently about 25k). Each <em>ruleset</em> is an XML file containing information about upgrading requests to HTTPS for a domain or group of domains (e.g. for an organization like Bitly). The file can contain the following entities:</p><ul><li><strong>Targets</strong>—define which domains are targeted by this ruleset (e.g. <code>example.com</code>). They can also make use of wildcards, either to target all subdomains, or multiple top-level domains.</li><li><strong>Exclusions</strong>—are regular expressions allowing to prevent some specific domains or URLs from being upgraded to HTTPS (e.g. to prevent breakage).</li><li><strong>Rules</strong>—define how insecure requests should be upgraded <em>from</em> insecure <em>to</em> secure (i.e. they encode the URL rewriting logic). They define a <code>from</code> regular expression which the input URL should match, associated with a <code>to</code> attribute describing how the upgraded URL should look like. The most common case being to simply transform <code>^http:</code> into <code>https:</code>.</li><li><strong>Secure Cookies</strong>—Optionally defined if cookies from one of the targeted domains should be secured as well using hardened flags.</li></ul><p>Here is a simple example of how such a ruleset could look like:</p><pre class="code" data-lang="xml"><code><span class="hljs-tag">&lt;<span class="hljs-name">ruleset</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;example.com&quot;</span>&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">target</span> <span class="hljs-attr">host</span>=<span class="hljs-string">&quot;example.com&quot;</span> /&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">target</span> <span class="hljs-attr">host</span>=<span class="hljs-string">&quot;*.example.com&quot;</span> /&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">exclusion</span> <span class="hljs-attr">pattern</span>=<span class="hljs-string">&quot;^http://unsecure\.example\.com/&quot;</span> /&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">rule</span> <span class="hljs-attr">from</span>=<span class="hljs-string">&quot;^http:&quot;</span> <span class="hljs-attr">to</span>=<span class="hljs-string">&quot;https:&quot;</span> /&gt;</span>
    <span class="hljs-tag">&lt;<span class="hljs-name">securecookie</span> <span class="hljs-attr">host</span>=<span class="hljs-string">&quot;.+&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;.+&quot;</span> /&gt;</span>
<span class="hljs-tag">&lt;/<span class="hljs-name">ruleset</span>&gt;</span>
</code></pre><h2>Matching Algorithm</h2><p>Given a collection of rulesets, the decision of upgrading an insecure request to a secure one relies on the following steps:</p><ol><li>Identifying the subset of rulesets <em>targeting</em> the URL’s domain.</li><li>Eliminating rulesets having at least one matching <em>exclusion</em> rule.</li><li>Evaluating the <em>rules</em> from the candidate rulesets until one matches.</li></ol><p>If a matching <code>rule</code> is found following the previous steps, then the URL is rewritten to a secure version using the rewriting logic defined by this <code>rule</code>. For more information about the exact semantic of matching rulesets, check <a href="https://www.eff.org/https-everywhere/rulesets" target="_blank" rel="noopener noreferrer">this page</a> of the official documentation.</p><h2>Efficient Matching</h2><p>The naive approach to writing a matching algorithm would be to iteratively inspect all rulesets for each input URL, checking their <em>targets</em>, <em>exclusions</em> and <em>rules</em> until a match is found. This would not be very efficient and we can do better (to be clear, this is <em>not</em> the approach taken by HTTPS Everywhere and I only describe it to get a sense of the most naive solution).</p><p>In the following few sections we are going to explore some of the biggest ideas contributing to the speed and memory efficiency of the new matching engine. Firstly, I will present the central <em>indexing data structure which allows to drastically reduce the amount of work required</em> to find relevant rulesets. Secondly, I will walk you through <em>how this index can be represented in a very compact way</em>, as a single typed array. Thirdly, we will see how we can further reduce the memory usage by implementing a built-in string compression capability to this compact index. Lastly I will briefly describe two other attempts at reducing the size of the index using a trie data structure and an experimental probabilistic data structure based on hashing.</p><h3>Reverse Index</h3><p>Instead of iterating through all rulesets for each input URL, we want to quickly identify a small subset of candidates which will be evaluated against the input URL. To achieve this goal, we rely on a <a href="https://github.com/remusao/https-everywhere-core/blob/master/src/reverse-index.ts" target="_blank" rel="noopener noreferrer">reverse index</a> which groups <em>targets</em>, <em>exclusions</em> and <em>rules</em> into buckets indexed by a common substring (or <em>token</em>) that they contain. This allows us to collect candidates for a given URL by querying the index with tokens found in the URL. Each candidate retrieved is thus guaranteed to share at least a common substring with the URL. In practice, this drastically reduces the amount of work required to take a decision. This technique is <a href="https://0x65.dev/blog/2019-12-20/not-all-adblockers-are-born-equal.html#CurrentApproach" target="_blank" rel="noopener noreferrer">used as part of content blockers</a> to identify lists of filters indicating that a network request should be canceled.</p><p>We create a separate index for <em>targets</em>, <em>exclusions</em>, <em>rules</em> and <em>secure cookies</em>. To minimize the number of candidates retrieved for each URL, we make sure that each <em>target</em> and <em>secure cookie</em> is indexed using <em>its rarest token</em>, whereas <em>exclusions</em> and <em>rules</em> are indexed only using the ID of the ruleset they belong to. In practice, each index is created using the following algorithm:</p><ol><li>Each element is tokenized using <code>\w+</code> (alpha-numeric characters) or the ruleset ID is used as a token. For example <em>target</em> <code>example.com</code> would be tokenized into <code>['example', 'com']</code>.</li><li>We keep track of the number of occurrences of each token with a global counter.</li><li>We then select the <em>best</em> (i.e. least seen) token for each element, and use it as a key in the reverse index.</li></ol><p>As a result, most <em>buckets</em> of the index will contain a single element (meaning that we found a token which is unique globally to index the element). To get a better idea of the dispatching capabilities brought by this technique, consider the following statistics collected by matching <a href="https://cdn.cliqz.com/adblocking/requests_top500.json.gz" target="_blank" rel="noopener noreferrer">a datasets</a> containing 240k URLs from the most popular domains against the HTTPS Everywhere rulesets:</p><ul><li>The median number of <em>targets</em> candidates evaluated for a given URL is: <strong>7</strong>—from a total of 163k; which means we only need to look at 0.004% of all <em>targets</em> on average. And out of these targets, most only cost a look-up in a <code>Set</code> since we often get multiple candidates from the same ruleset. By keeping track of which rulesets we are already considering, we only need to evaluate the first target from a given ruleset. The median number of <em>targets</em> candidates requiring a string comparison is: <strong>5</strong>.</li><li>The median number of <em>rulesets</em> considered is: <strong>1</strong>, with a maximum of: <strong>2</strong> in the rare case where a given domain is targeted by more than one ruleset (from a total of 25k).</li><li>For each ruleset, we then retrieve a combined exclusion (all regular expressions aggregated into one, joined with <code>|</code> characters), resulting in one or two <code>RegExp</code> evaluations (from the one or two rulesets considered).</li><li>Finally, we inspect the <em>rules</em> from each ruleset not already excluded, until we find a match. The median number of <em>rules</em> considered is <strong>2</strong>.</li></ul><figure><a href="../images/posts/efficient-https-everywhere-engine/speed.svg"><img src="../images/posts/efficient-https-everywhere-engine/speed.svg" alt="" loading="lazy"></a><figcaption>Average decision time per URL.</figcaption></figure><p>This graph depicts the average time it takes to rewrite a request to HTTPS, based on the latest snapshots of rulesets, evaluated against the dataset of 240k URLs mentioned above. Please note that internal caching of HTTPS Everywhere <em>was disabled</em> for these measurements, to only take into account the raw speed of the engine.</p><p>It is surprising to observe that the Rust/WebAssembly version is slower than the JavaScript implementation. Although both are really fast since even the “slowest” result is of <strong>0.028 milliseconds</strong> on average. It could very well be that the overhead of transferring data from JavaScript to WebAssembly is responsible for this result. On the other hand, we see that our reverse index implementation is faster than both, with an average time between <strong>0.0046</strong> and <strong>0.0073 milliseconds</strong>. Note that running the same benchmark in Node.js results in an even faster decision time of <strong>0.0029 milliseconds</strong>—this might be explained by the fact that browsers are less friendly to benchmarking due to the many components potentially competing for CPU resources, but this is just speculation on my part.</p><h3>Binary Representation</h3><p>While the indexing technique described in the previous section speeds-up matching drastically, it is not optimal in terms of memory usage and initialization time. If the index is represented as a <code>Map</code>, it means that on each initialization (when the extension starts) we need to either re-create the index from scratch (using the raw XML rulesets or a JSON version of it), or load it from a textual representation of the <code>Map</code> (i.e. from cache), like an array of <em>key</em>, <em>value</em> pairs.</p><p>Instead, the reverse index described above is implemented as a compact binary data structure stored in a single <code>Uint8Array</code> (typed array), where the data is organized in a way that allows for efficient look-ups. In-memory instances of <em>targets</em>, <em>exclusions</em>, <em>rules</em> and <em>secure cookies</em> along with the instances of <code>RegExp</code> required to match against input URLs and domains are only lazily loaded and compiled from their binary representation stored in the typed array, when there is a chance they will match, thanks to the reverse index. These instances can also be (optionally) cached into a <code>Map</code> so that subsequent look-ups do not need to hit the binary index (which is a bit slower than <code>Map.get</code>).</p><p>Since the number of rulesets really considered in practice is more or less proportional to the number of unique domains visited by a user during a browsing session, the additional memory usage required for the caching mechanism is fairly small.</p><p>More implementation details are given in the section “Going low-level with typed arrays” from <a href="https://0x65.dev/blog/2019-12-20/not-all-adblockers-are-born-equal.html" target="_blank" rel="noopener noreferrer">this other article</a>. To summarize the benefits of this data structure:</p><ul><li>It allows to encode all rulesets into a very compact, binary format, stored in a single <code>Uint8Array</code>. The total memory usage of the extension using such an engine is therefore fairly predictable, and close to the size of this typed array.</li><li>Serialization and deserialization are extremely efficient since the look-ups can be performed directly on this <code>Uint8Array</code> instance without the need to first copy the data into a more convenient data structure such as a <code>Map</code>. Serialization thus consists in storing the same typed array locally (e.g. in IndexedDB), and deserialization consists in reading it back.</li><li>This binary data structure can be created once on the server-side and hosted on a CDN, so that clients can fetch it directly, speeding-up initialization further (The following <a href="https://github.com/remusao/https-everywhere-core/blob/master/engine.bin" target="_blank" rel="noopener noreferrer">binary file</a> is updated automatically using a GitHub Workflow triggered using <code>cron</code>).</li><li>In-memory instances of <em>targets</em>, <em>exclusions</em>, <em>rules</em> and <em>secure cookies</em> along with the instances of <code>RegExp</code> required to match against input URLs and domains are only lazily loaded and compiled from the binary representation, when there is a chance they will match, thanks to the reverse index.</li></ul><p>The drawback of this approach, though, is that updating (adding or deleting elements from the index), currently requires to recreate the index completely, which is relatively costly (it takes around <em>500 milliseconds</em>). But since updates can be performed backend-side, and are relatively infrequent, this is not a road-blocker.</p><figure><a href="../images/posts/efficient-https-everywhere-engine/init.svg"><img src="../images/posts/efficient-https-everywhere-engine/init.svg" alt="" loading="lazy"></a><figcaption>Average initialization time for rulesets.</figcaption></figure><p>These measurements offer some fairly surprising results. On the one hand we see that Chrome seems to struggle with the WebAssembly version compared to Firefox. Once again the JavaScript implementation outperforms the Rust/WebAssembly combo, though. Again, my best guess is that the amount of data transfered to WebAssembly context might be partly responsible for that, but I would love to get more insights from core developers on this. We also see that our binary index is extremely fast to initialize, between <strong>12</strong> and <strong>45 milliseconds</strong>. We could reduce it further by disabling the built-in <em>CRC-32</em> checksum mechanism which ensures that the buffer is not corruption before deserializing it, but this does not seem necessary.</p><h3>String Compression</h3><p>Up to this point, we have shown how we can efficiently query rulesets and how the indexing data structures can be represented in a compact way, friendly to serialization and deserialization to allow faster initializations. The size of the final typed array is roughly of <strong>7MB</strong>. When looking closer, it appears that a big proportion of this data consists of the raw strings from <em>targets</em> (<code>host</code>), <em>exclusions</em> (<code>pattern</code>), <em>rules</em> (<code>from</code> and <code>to</code>), as well as <em>secure cookies</em> (<code>host</code> and <code>name</code>): about <strong>3MB</strong>, or a bit more than 40% of the total size.</p><p>Looking at these strings, it does not take long to notice that some values are very frequent, like <code>.+</code> in <em>secure cookies</em>, or <code>^http:</code> and <code>https:</code> in <em>rules</em>. One way to take advantage of these patterns would be to hard-code the detection of some of the common strings and replace them by opcodes, or perform some kind of <a href="https://en.wikipedia.org/wiki/String_interning" target="_blank" rel="noopener noreferrer">string interning</a>, to avoid having many times the same data in memory (or in the compact reverse index).</p><p>Another way to look at the problem, which could also be seen as a string interning mechanism, is to rely on a codebook-based compression algorithm to reduce the size of strings. It so happens that I had already experimented with such techniques in the past (e.g. using <a href="https://github.com/antirez/smaz" target="_blank" rel="noopener noreferrer">SMAZ</a> or <a href="https://ed-von-schleck.github.io/shoco/" target="_blank" rel="noopener noreferrer">shoco</a>). I ended up implementing a custom variant of SMAZ in pure-JavaScript to integrate into the <a href="https://github.com/cliqz-oss/adblocker" target="_blank" rel="noopener noreferrer">adblocker</a> I was working on. The library offers an <a href="https://github.com/remusao/mono/tree/master/packages/smaz-generate#remusaosmaz-generate" target="_blank" rel="noopener noreferrer">automatic codebook-generation function</a> that tries to find optimal codebooks based on a list of input strings.</p><p>Applying this codebook compression idea to rulesets, we are able to compress strings by 40 to 60%, further reducing the total size of the serialized engine to <strong>5MB</strong> (i.e. a <em>2MB</em>, or <em>30%</em>, reduction). Applying this optimization can be done transparently in the custom DataView-like <a href="https://github.com/remusao/https-everywhere-core/blob/master/src/data-view.ts" target="_blank" rel="noopener noreferrer">abstraction</a> used to serialize data to the binary representation and back.</p><p>A drawback of relying on codebooks is that they need to be re-generated when the rulesets are updated so that they remain relevant. The <a href="https://github.com/remusao/https-everywhere-core/blob/master/.github/workflows/rulesets.yml" target="_blank" rel="noopener noreferrer">prototype hosted on GitHub</a> is relying on a GitHub Workflow to update the codebooks based on the latest snapshot of the rules and open a PR with the updated assets. The codebooks are also shipped as part of the binary representation of the matching engine, which means that clients downloading a new version from the CDN (i.e. GitHub) always get the best compression, without needing to update the source code.</p><figure><a href="../images/posts/efficient-https-everywhere-engine/memory.svg"><img src="../images/posts/efficient-https-everywhere-engine/memory.svg" alt="" loading="lazy"></a><figcaption>Memory usage with strings compression.</figcaption></figure><p>This plot shows the size occupied by rulesets as reported by the Chrome Memory Dev Tool with a snapshot. We see that the memory usage went down with WebAssembly compared to the initial JavaScript implementation of HTTPS Everywhere. On the other hand, our binary index uses less memory, and the difference is even bigger when using string compression as well.</p><h3>Do. Or do not. There is no Trie.</h3><p>Although the codebook-based compression is very effective at reducing the memory usage of raw strings needed for matching rulesets, there might be more efficient approaches depending on the nature of the data. In particular, <em>targets</em> are domain names, most of which are not using wildcards at all; they also represent the bulk of the strings. A <a href="https://en.wikipedia.org/wiki/Trie" target="_blank" rel="noopener noreferrer">trie</a> is commonly used to represent this kind of data. We expect suffixes of domains to be repeated among many targets (some top-level domains are very common).</p><p>I already knew it was possible to encode a trie in a very compact way—using only one 32-bit number to represent each node when storing ASCII strings. Before putting the work to implement this new data structure, I started by estimating the expected final size to make sure it was worth it.</p><p>I constructed the trie in-memory using a more naive representation based on JavaScript objects and an instance of <code>Map</code> in each node to link a parent to its children. Storing all <em>targets</em> resulted in a trie of <code>1,654,430</code> nodes, which would result in about <strong>6.6MB</strong> of memory with our compact representation. Not very encouraging…</p><p>I then realized that it would probably make more sense to store the domains in reverse, to benefit from compression of top-level domains. After reversing the order of <em>targets</em> on insertion, the number of nodes went down to <code>878,251</code>, which would result in <strong>3.5MB</strong> of memory. This already seemed more reasonable. But we also need to factor-in the extra information about which ruleset each <em>target</em> belongs to (information needed when matching). Given that we have <code>163,486</code> <em>targets</em>, and assuming we find a way to encode the ruleset membership with an extra 32-bit number for each target, a back-of-the-envelope calculation tells us that we would need an extra <em>650KB</em>, resulting in a total of <strong>4.1MB</strong> memory usage. Even assuming a very optimistic 16-bit overhead per target, we would still need more memory to store <em>targets</em> than with the codebook-compression approach described above.</p><p>This concluded the experimentation with tries. Unfortunately, it does not seem like using a trie would yield any significant savings compared to the string compression method already implemented. It might be a viable option if string compression is not to be implemented at all. Also, it could be that better results can be obtained using a more advanced trie structure such as a <a href="https://en.wikipedia.org/wiki/Radix_tree" target="_blank" rel="noopener noreferrer">patricia</a> or <a href="https://db.in.tum.de/~leis/papers/ART.pdf" target="_blank" rel="noopener noreferrer">adaptive (ART)</a> trie, which would allow to store multiple characters into a single node.</p><h3>Compact Hashes</h3><p>At this point it seemed like the different new ideas to improve the memory representation of rulesets were hitting diminishing returns. As a last trick, I thought of implementing a data structure which allows to trade space for uncertainty (a.k.a. probabilistic data structure). I had already experimented with a similar approach when working on the <a href="https://github.com/remusao/tldts#tldts---blazing-fast-url-parsing" target="_blank" rel="noopener noreferrer"><code>tldts-experiment</code></a> package. You are probably familiar with <a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank" rel="noopener noreferrer">Bloom filters</a>, which allow to perform membership tests on a potentially large collection of elements, while keeping the memory usage under control (by adjusting the probability of false positives).</p><p>Instead of going for full-blown Bloom filters, I decided to experiment with a simpler method, also based on hashing. The idea is fairly simple, each domain in the collection is stored in a bucket alongside domains having the same number of labels. Each bucket is a sorted array of 32-bit hashes of these domains. We could also store all hashes in a single array regardless of the number of labels, but this increases the probability of collisions by a <em>factor of 2</em>. Each bucket is followed by a second array of same size, containing ids of rulesets to which the targets belong. The final data structure is then composed of all buckets concatenated into a single <code>Uint32Array</code>.</p><p>Using <a href="https://github.com/remusao/https-everywhere-core/blob/master/src/hashes.ts" target="_blank" rel="noopener noreferrer">this trick</a> allows to reduce the size of the final serialized engine to <strong>2.1MB</strong> (i.e. a <em>2MB</em>, or <em>40%</em>, reduction). This comes at the cost of a <code>0.000017</code> probability of collision when looking-up <em>targets</em> in the index (estimated using <a href="https://github.com/duckduckgo/smarter-encryption/blob/master/README.md#just-want-the-list" target="_blank" rel="noopener noreferrer">a list of 20M popular domains</a>). For this reason, this feature is turned-off by default but can be enabled using a config flag when building the binary engine.</p><p>Note that an extra <em>320KB</em> of memory could be saved by using a 16-bit identifier for ruleset IDs instead of the current 32-bit identifier (which could work because we only have 25k rulesets at the moment and this can be represented using 16-bit numbers). This would reduce the total memory usage to <strong>1.8MB</strong> of memory (i.e. a 10x improvement over the memory usage of the current HTTPS Everywhere implementation in Rust compiled to WebAssembly).</p><h2>Conclusion and Future Work</h2><p>In this article I have presented the current state of an experiment aiming at implementing a more efficient matching engine for HTTPS Everywhere rulesets. Using a radically different design, matching can be made <strong>between 4x and 10x more memory-efficient</strong>, initialization of the engine reduced to less than <strong>25 milliseconds</strong>, and HTTPS upgrades performed in <strong>0.0029</strong> to <strong>0.0073 milliseconds</strong>, without relying on the Rust/WebAssembly combo.</p><p>The source code can be <a href="https://github.com/remusao/https-everywhere-core" target="_blank" rel="noopener noreferrer">found on GitHub</a>. You can also install a simple WebExtension and try out the new engine locally in Firefox or Chromium.</p><p>There are currently a few known limitations compared to the official HTTPS Everywhere:</p><ul><li>No way for users to add custom rules. This can be implemented as a second, smaller engine which would be stored separately from the main rulesets.</li><li>Rulesets need to have a unique 32-bit identifier known when creating the index. For built-in rules, this ID is determined whenever the engine is created. To handle user-defined rules, we could either rely on a counter maintained client-side, or a 32-bit hash of the ruleset name.</li><li>Metadata for rulesets (i.e. the name and default state) are currently discarded at build-time. It would be fairly easy to include them in the engine for an estimated overhead of <em>350KB</em> in the final size. It should be noted, though, that the <code>name</code> information is currently not needed in the prototype.</li><li>Rulesets marked as <em>mixedcontent</em>—only supported in the Tor browser—are discarded at build time. They could easily be included, either enabled by default (if we ship two different engines, fetched by clients depending on their browser support), or side-by-side with the other rules and enabled dynamically.</li></ul><p>Lastly, I was surprised to observe that the Rust/WebAssembly version of the engine as currently shipped in HTTPS Everywhere seems to be slower at both initialization and operating than the previous JavaScript implementation. This seems to contradict the <a href="https://twitter.com/eff/status/1172622942158479360" target="_blank" rel="noopener noreferrer">previous claims</a> when this was <a href="https://twitter.com/HTTPSEverywhere/status/1144375820334424064" target="_blank" rel="noopener noreferrer">first released</a>. I’d love to get some feedback from core developers about this potential issue and more insights from them to understand why I got these results.</p><p>I hope this work will be helpful to the community and I would be glad to discuss these findings in more details with people directly working on the HTTPS Everywhere extension.</p></div><div class="comments"><span><a class="leave-comment-btn" href="https://github.com/remusao/remusao.github.io/issues/248" title="https://github.com/remusao/remusao.github.io/issues/248" target="_blank" rel="noopener noreferrer">Leave a comment on GitHub</a></span><details><summary>1 comment</summary><ul class="comments-list"><li><div class="comment"><div class="meta"><img loading="lazy" class="avatar" src="/images/comments/avatar/57873259.jpg" width="40" height="40"> <a class="author" href="https://github.com/gundayioglu" title="gundayioglu" target="_blank" rel="noopener noreferrer">gundayioglu</a> <span>commented </span><a class="date" href="https://github.com/remusao/remusao.github.io/issues/248#issuecomment-687143512" title="https://github.com/remusao/remusao.github.io/issues/248#issuecomment-687143512" target="_blank" rel="noopener noreferrer">3 hours ago</a></div><div class="content"><p>Great post! I love how this implementation results in such gains in speed and memory usage. It shows how changing the language might not <em>always</em> be the best first-action when trying to optimize.</p></div></div></li></ul></details></div></article><footer><div class="share"><ul><li><a href="https://www.facebook.com/sharer/sharer.php?u=https://remusao.github.io//posts/efficient-https-everywhere-engine.html&t=A more efficient matching engine for HTTPS Everywhere RuleSets" title="Share on Facebook" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="Share on Facebook" src="/images/social_flat_rounded_rects_svg/Facebook.svg"><a></a></li><li><a href="https://twitter.com/share?url=https://remusao.github.io//posts/efficient-https-everywhere-engine.html&text=A more efficient matching engine for HTTPS Everywhere RuleSets&via=Pythux" title="Tweet" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="Tweet" src="/images/social_flat_rounded_rects_svg/Twitter.svg"><a></a></li><li><a href="https://getpocket.com/save?url=https://remusao.github.io//posts/efficient-https-everywhere-engine.html&title=A more efficient matching engine for HTTPS Everywhere RuleSets" title="Add to Pocket" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="Add to Pocket" src="/images/social_flat_rounded_rects_svg/Pocket.svg"><a></a></li><li><a href="https://news.ycombinator.com/submitlink?u=https://remusao.github.io//posts/efficient-https-everywhere-engine.html&t=A more efficient matching engine for HTTPS Everywhere RuleSets" title="Submit to Hacker News" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="Submit to Hacker News" src="/images/social_flat_rounded_rects_svg/HackerNews.svg"><a></a></li><li><a href="https://www.reddit.com/submit?url=https://remusao.github.io//posts/efficient-https-everywhere-engine.html&title=A more efficient matching engine for HTTPS Everywhere RuleSets" title="Submit to Reddit" target="_blank" rel="noopener noreferrer"><img loading="lazy" alt="Submit to Reddit" src="/images/social_flat_rounded_rects_svg/Reddit.svg"><a></a></li></ul></div></footer></main></body></html>